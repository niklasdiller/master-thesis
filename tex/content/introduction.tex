\chapter{Introduction} \label{chap:introduction}

% TODO Disclaimer hinzufügen? : Die "gleiche" Variablen manchmal mit anderer schreibweise, weil sie in anderem kontext gemeint sind? zB window_size in DB und windoSize in Java. 

A common and widely known challenge in training and using machine learning models is to find a suitable balance between overfitting and underfitting \cite{vanderaalst2010}. The goal for coming up with valuable predictors for a specific problem therefore is often to create models that neither simply reproduce results gathered from the training data, nor make too broad generalizations. Overfitting would mean that the model would not only learn the underlying pattern of data but also random noise that comes with it. This results in a bad-performing model for new and unseen data, as the model is constantly trying to match its’ predictions with the training data too closely. Underfitting on the other hand happens if a model fails to completely cover the complexity of a specific problem. Because new data points are unfamiliar to the model, the resulting predictions will also be poor and not accurate \cite{montesinoslopez2022}.  It becomes a crucial task for machine learning engineers to balance the amount of the training data and the hyperparameters for machine learning models to find a suitable golden mean of creating models that correctly represent the underlying patterns but is at the same time capable of handling new data points. 

A similar crucial balancing act in machine learning that seems to find much less attention, however, is the equipoise of the concepts of performance and resource awareness. Both principles are essential for a valuable machine learning model: Predictors need to produce accurate results in order to be of value and research has laid an increasing focus on designing resource-aware machine learning systems \cite{rapp2022}. While the two concepts don’t necessarily cancel each other out, there seems to be a trade-off between them when exploring and training models. Good-performing models often make use of a big set of features and complex algorithms and in order to produce a stream of up-to-date predictions, they are often called in short time intervals. Machine learning models of this kind, however, tend to demand a larger number of computational resources, e.g. CPU time, memory consumption, or network utilization. When only looking for the best performing model, i.e. the model that produces the most accurate results, the predictions might become too expensive in regards to memory or battery usage, or simply too computationally complex \cite{preuveneers2020}. Especially in the context of edge computing, this balance between good performance and good resource utilization becomes crucial. Edge computing is a computing paradigm that focuses on processing data at the same place where it is produced: at the edge of the network. In contrast to cloud computing, which works by sending all raw data to a central node to be processed there, edge computing operates on the data near the sensors that produce it. That way, less data needs to be sent on an anyway limited network bandwidth, accounting for faster response times, and less network pressure \cite{shi2016}.

The main motivation for this work are two projects that the Chair of Mobile Systems of the University of Bamberg is carrying out: A classification problem on the one hand, that aims to correctly categorize the behavior of cattle using sensors, and a regression problem on the other hand, in which the availability of parking spaces is to be predicted. Both projects raise the demand for a model set retrieval system that lets the user decide, how significant resource awareness is supposed to be in relation to performance. These projects will be introduced in greater detail in \autoref{chap:relatedwork}. For a model retrieval system to work, several varying machine learning models need to exist first. It was therefore decided to implement a model training pipeline along with the retrieval system itself. The selection of the best model sets will then be done using a Python module consisting of two parts: First, the best single models will be retrieved using weights that are specified by the user. In the second part, the models are then combined to create model sets, out of which the best are retrieved as well before they are finally returned to the user. The centerpiece of each iteration of this retrieval process is the underlying top-k algorithm. This work introduces a naïve approach to selecting the most fitting models by looking at each entry in the database. In addition, however, two more refined algorithms will be introduced, namely Fagin's Algorithm (FA) and Threshold Algorithm (TA). These approaches make use of different modes of data access to retrieve the top $k$ models in a faster and more efficient way. FA and TA will be presented more thoroughly in \autoref{chap:relatedwork} as well.

This work will focus on answering the question of how to manage a meaningful balance between performance and resource awareness in the model selection process. In particular, the creation of ensembles or model sets will be discussed from this perspective while applying them to the use cases that this work will refer to. As questions regarding the conceptualization of different parts of the use cases are still open, this work also aims to elaborate various theoretical ideas and concepts regarding resource awareness, prediction horizons, and model scores.

To answer this research question, the concept of resource awareness will be made quantifiable by splitting it up into two components that will be established in this work: Intra- and inter-model resource awareness. With the help of this distinction, the extent of efficient resource utilization can be made tangible through different scores before and after the creation of model sets. Combining these resource awareness scores with scores about performance, a final grade can be assigned to each model set which then is used for the actual top-k retrieval. The developed scores and the highly customizable way of their formation prove to be the main contributions of this work. Having the option to choose between different efficient top-k algorithms rounds off the overall attempt to make the model selection process more resource-aware. 

The chapters of this thesis are structured as follows: First, the related research of this work will be presented in \autoref{chap:relatedwork} by introducing several theoretical concepts that serve as a foundation for the upcoming chapters. Thus, model ensembles, resource awareness, performance, the top-k retrieval process as well as the previously mentioned projects that serve as a motivation for this work are explained. \autoref{chap:design} then will present the assumptions, concepts, and overall design of the training pipeline and the top-k retrieval system that were developed. Following, \autoref{chap:implementation} will be devoted to a more practical manner, by going over the decisions that were during the implementation phase of this project. Here, various attributes that were used to store the created models are introduced, as well as metrics that were continuously changed during the execution of the training pipeline. In addition, it will be detailed how the retrieval system works and what strategy was chosen to depict the time difference between prediction and time-to-predict. \autoref{chap:evaluation} subsequently evaluates the implemented modules by looking at different metrics like execution time and number of accesses of the top-k algorithms and network utilization for resource-aware model sets. These findings are then finally summarized in \autoref{chap:conclusion} which also proposes several opportunities for future work.