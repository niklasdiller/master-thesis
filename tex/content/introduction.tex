\chapter{Introduction} \label{chap:introduction}

% TODO Disclaimer hinzufügen? : Die "gleiche" Variablen manchmal mit anderer schreibweise, weil sie in anderem kontext gemeint sind? zB window_size in DB und windoSize in Java. 

A common and widely known challenge in training and using machine learning models is to find a suitable balance between overfitting and underfitting \cite{vanderaalst2010}. The goal for coming up with valuable predictors for a specific problem therefore is often to create models that neither simply reproduce results gathered from the training data, nor make too broad generalizations. Overfitting would mean that the model would not only learn the underlying pattern of data but also random noise that comes with it. This results in a bad-performing model for new and unseen data, as the model is constantly trying to match its’ predictions with the training data too closely. Underfitting on the other hand happens if a model fails to completely cover the complexity of a specific problem. Because new data points are unfamiliar to the model, the resulting predictions will also be poor and not accurate \cite{montesinoslopez2022}.  It becomes a crucial task for machine learning engineers to balance the amount of the training data and the hyperparameters for machine learning models to find a suitable golden mean of creating models that correctly represent the underlying patterns but is at the same time capable of handling new data points. 

A similar crucial balancing act in machine learning that seems to find much less attention, however, is the equipoise of the concepts of performance and resource awareness. Both principles are essential for a valuable machine learning model: Predictors need to produce accurate results in order to be of value and research has laid an increasing focus on designing resource-aware machine learning systems \cite{rapp2022}. While the two concepts don’t necessarily cancel each other out, there seems to be a trade-off between them when exploring and training models. Good-performing models often make use of a big set of features and complex algorithms and in order to produce a stream of up-to-date predictions, they are often called in short time intervals. Machine learning models of this kind, however, tend to demand a larger number of computational resources, e.g. CPU time, memory consumption, or network utilization. When only looking for the best performing model, i.e. the model that produces the most accurate results, the predictions might become too expensive in regards to memory or battery usage, or simply too computationally complex \cite{preuveneers2020}. Especially in the context of edge computing, this balance between good performance and good resource utilization becomes crucial. Edge computing is a computing paradigm that focuses on processing data at the same place where it is produced: at the edge of the network. In contrast to cloud computing, which works by sending all raw data to a central node to be processed there, edge computing operates on the data near the sensors that produce it. That way, less data needs to be sent on an anyway limited network bandwidth, accounting for faster response times, and less network pressure \cite{shi2016}.

The main motivation for this work are two projects that the Chair of Mobile Systems of the University of Bamberg is carrying out: A classification problem on the one hand, that aims to correctly categorize the behavior of cattle using sensors, and a regression problem on the other hand, in which the availability of parking spaces is to be predicted. Both projects raise the demand for a model set retrieval system that lets the user decide, how significant resource awareness is supposed to be in relation to performance. These projects will be introduced in greater detail in \autoref{chap:relatedwork}. For a model retrieval system to work, several varying machine learning models need to exist first. It was therefore decided to implement a model training pipeline along with the retrieval system itself. As many questions regarding the conceptualization of different parts of the use cases are still open, this work also aims to elaborate various theoretical ideas and concepts regarding resource awareness, prediction horizons, and model scores.

This work will focus on answering the question of how to manage a meaningful balance between performance and resource awareness in the model selection process. In particular, the creation of ensembles or model sets will be discussed from this perspective while applying them to the use cases that this work will refer to. 

The chapters of this thesis are structured as follows: First, the related research of this work will be presented in \autoref{chap:relatedwork} by introducing several theoretical concepts that serve as a foundation for the upcoming chapters. Thus, model ensembles, resource awareness, performance, the top-k retrieval process as well as the previously mentioned projects that serve as a motivation for this work are explained. \autoref{chap:design} then will present the assumptions, concepts, and overall design of the training pipeline and the top-k retrieval system that were developed. Following, \autoref{chap:implementation} will be devoted to a more practical manner, by going over the decisions that were during the implementation phase of this project. Here, various attributes that were used to store the created models are introduced, as well as metrics that were continuously changed during the execution of the training pipeline. In addition, it will be detailed how the retrieval system works and what strategy was chosen to depict the time difference between prediction and time-to-predict. \autoref{chap:evaluation} subsequently evaluates the implemented modules by looking at different metrics like execution time and number of accesses of the top-k algorithms and network utilization for resource-aware model sets. These findings are then finally summarized in \autoref{chap:conclusion} which also proposes several opportunities for future work.


% TODO Einarbeiten:
%Du musst keine Angst haben bzgl. Wiederholungen (aber bitte kein CopyPaste ;-)) im Abstract und in der Introduction. Die Introduction sollte auf jeden Fall ausführlicher sein als der Abstract. Du hast aber einige Details im Abstract angemerkt, die in der Einleitung nicht erwähnt werden (bzgl. TopK Algorithmen, Scores, QSL, Evaluation, ...).

%Deine Einleitung ist sehr gut, was ich aber noch ergänzen würde:
%- Informationen bzgl. existierender Ansätze, wie z.B. TopK Algorithmen FA und TA
%- Informationen bzgl. deiner Contribution! Im letzten Abschnitt stellst du die Hauptfragestellung/Forschungsfrage vor, was sehr gut ist! Danach könntest du auch noch vorstellen, wie du diese Frage in deiner Arbeit bearbeiten/beantworten wirst. Also welche Schritte du durchgeführt hast und deren Hauptergebnisse. In diesem Rahmen kannst du deine Main-Contributions vorstellen. Intra- und Inter-Model Resource Awareness, Scores, QSL (eigentlich alles was das Scoring bzgl. TopK-Retrieval betrifft)
 