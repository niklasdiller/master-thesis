\chapter{Future Work and Conclusion} \label{chap:conclusion}

This work made various contributions to the model training as well as the model set retrieval process. These contributions range from enhancing already existent modules, for example by separating the preprocessing process for model training from the actual training pipeline, to designing and implementing new ideas, for example by introducing intra-model resource awareness as a counterpart for QSLs. The impacts of the constructed retrieval system along with its top-k algorithms have been evaluated on multiple metrics to show their benefits regarding the model set retrieval process, which met the scope of this work. Still, there are many aspects surrounding this work that should be addressed in future works in order to apply changes to the current system, evaluate different alternatives and design choices regarding the architecture of this project, and ultimately improve the current implementation of both model training pipeline and model set retrieval system. This chapter aims to both refer to areas that can be used for further work on the topics of this work, as well as to summarize the undertakings and impacts of this thesis.

\section{Future Work}

The entire assessment of both performance and resource awareness of models and model sets in this work has been done in the context of the model training and model selection process. Even in analyzing the number of accesses per top-k algorithm, the evaluation still takes place “before” any model is run in production, i.e. outputting any predictions. For a completely comprehensive evaluation of any model, however, it is crucial to understand its performance inside a complete and working environment. Therefore, future work should aim to locate and assess suitable metrics that refer to the models' ability to produce valuable predictions. One possible metric could be the inference time of a model. Evaluating, how much time a model takes to generate an output is a relevant step, especially when working with small window sizes and prediction horizons. Assuming a certain model is supposed to produce a prediction every minute, requires the model to take significantly less time than a minute to produce said prediction, otherwise its informative value would be of no use. This additionally applies to when working with small prediction horizons. This work produced model sets with the smallest prediction horizon being 10 minutes. If potential future work includes much smaller prediction horizons, however, e.g. 2 minutes, a long inference time would significantly reduce the usefulness of a model. For these reasons, analyzing the inference time of models and integrating it into its scoring function when in the process of selecting models for model set creation, might be a valuable topic to work on in the future. An adjoining question to further pursue would also be, whether to account the inference time of a model toward its performance or resource awareness score. 

A metric that would definitely be valuable to contribute to a model's resource awareness score, however, is RAM usage. Potential follow-up work could integrate the number of bytes of RAM the training, storing, selecting, and in particular the inference of a certain model requires. Less RAM used up by one model means more available computational resources for other processes. Consequently, the demand for RAM of models correlates directly with its resource awareness. While RAM usage might already be indirectly addressed in this work when evaluating the number of accesses of the different top-k algorithms, the focus was not on the model itself but instead on the model set retrieval system itself. Also, the number of sorted or random accesses is probably not a reliable indication of RAM usage even though the two metrics might correlate. Thereby, a precise analysis of RAM usage might contribute to a more omniscient assessment of resource awareness as a whole.

As mentioned before, when working with a very large model database, the execution time the respective top-k algorithm takes up is relatively small when compared to the time the entire request takes. To make even complex requests faster, an optimization of the model set retrieval app is a major option for future work. Reducing overhead by making sure the most efficient data types or sorting algorithms are used, might make the retrieval process more user-friendly by taking up less time. As already mentioned, micro-optimizations in code were not the scope of this work and are therefore left for future development.

Another question left open by this work is the handling of the weather-related features when working with prediction horizons. Mentioned in an earlier chapter, the option to use data from a weather forecast was mentioned. As an example, let’s assume a model that is to predict the parking occupancy in an hours’ time (i.e. with a prediction horizon of 60 minutes) is using both temperature as well as humidity among other features. One option would now be to request forecasted weather data from an external source and use it as input for the model. If for example rain was forecasted, the model could use this increase in humidity and decrease in temperature to potentially make a more precise prediction about the parking occupancy. While in theory, this approach might seem valuable, there are various loose ends that might decrease the models’ quality when using weather forecasts. For one, the forecast might of course be wrong and would therefore shift the models’ prediction in the wrong direction. Secondly, reliable forecasts for small prediction horizons like 10 minutes might even be hard to come by or could cause a larger latency and data usage than just relying on current weather data. Especially for relatively small prediction horizons it might therefore make more sense to use current data on temperature and humidity than risking falsely shifted predictions by relying on forecasted data. However, this debate is a good starting point for any future work on this project. Comparing the performance and resource utilization of models using current weather data with models using weather forecasts might be a research question worth examining. 

Regarding the design of the scores that were made up for this project, many decisions, assumptions, and restrictions had to be made. Different considerations led to following certain ideas and having to neglect others. One of those disregarded ideas was to come up with global scores that make it possible to compare a model with any other model from the entire database, across different requests. In the current implementation, each score (single model scores like the Resource Awareness Score, as well as scores of a model set like the Aggregated Model Score) must be seen in the context of the underlying request. For instance, the Performance Score of a model that is predicting the occupancy for parking lot 38 using a prediction horizon of 60 minutes, cannot be compared to a model predicting for parking lot 634 on a prediction horizon of 10 minutes. The reason for this is, that each score is normalized using the best possible result (i.e. in this case the best possible accuracy value for example) inside a specific request. A comparison across different requests, and therefore across different parking lots, prediction horizons, and window sizes might very well be valuable: It makes different requests and use cases comparable with each other, making it apparent where the quality of models might still be lacking. Global scores like this could be implemented by looking at the metrics of every model in the entire model database, assessing their scores, and using them as the basis for normalization. Nevertheless, it was decided against the implementation of global scores. By comparing every model with each other, the scale values would significantly be shifted. As a result, differences e.g. in the Performance Score between models would suddenly seem to be marginal, because of potential outliers. While comparing models inside a defined context would make differences in performance or resource awareness apparent, using global scores could make these differences vanish. Keeping the scale in a limited but meaningful way when comparing models of the same request is what makes these comparisons valuable. Additionally, by returning the absolute value of the chosen performance metric gives the user at least some option for global comparison: While with the returned scores, a comparison across parking lots, prediction horizons, or window sizes is not possible, the returned absolute performance metric, e.g. the RMSE value is in fact suited for comparison in different contexts. 

One last additional idea that could be developed is a change in the aggregation function of the introduced scores. As shown by Fagin et al., using the minimum value of an object's properties as its’ overall score is a common practice \cite{fagin2002}. In the context of this work, this could mean that the Model Score of a model would always be whatever single-model score - Resource Awareness Score or Performance Score - is smaller. This restructuring would not only make the use of weights obsolete but would also come with another wide array of implications, which would have to be examined beforehand. Ultimately, this work offers a wide range of points to improve upon and to take as an opportunity to do further research. However, the implemented training pipeline and model set retrieval system, the designed metrics and scores as well as the conceptualized ideas brought up in this work can be considered as a valuable basis for the model selection and model ensemble process.

% While the proposed open questions in this section .... Eingehen, dass das hier disktutiert wurde (um diskussion mit drin zu haben quasi)?



\section{Conclusion}

This work elaborated the theoretical background of model set creation and retrieval and proposed both the design and implementation of a working model set retrieval system. In addition, a working model training pipeline was introduced, which acts as an important cornerstone in the subsequent creation of model sets. The showcased algorithms and ideas were then evaluated using different metrics such as the number of accesses, execution time, and network utilization.

The implemented preprocessing pipeline makes the overall model creation process much faster, by getting rid of redundancies and making sure each preprocessing step is only done as often as it is needed. By already having preprocessed data available, the training of models can be done much more flexibly and less time-consuming. Utilizing the Weka library, the developed training pipeline then allows the user to have a large number of machine learning models trained and stored, according to their preferences in metrics. The structure of the pipeline makes it easy to both change the different values each model metric should assume (e.g. “models with window sizes of 1 and 5 minutes should be trained”) as well as change the overall metrics that should show a variation (e.g. “apart from the window size, also the train-test-strategy for the models should change”). By combining the preprocessing and the training pipeline, the model creation process has been vastly automated and simplified. By setting up the desired metrics and their values and running the pipelines in the background, e.g. by using a virtual machine, a diverse model database with several thousand entries can be expected in only a few hours’ time.

The main part of this work is the developed model set retrieval system. Using the models that were created with the help of the training pipeline, various model sets can be created and eventually be compared to one another. According to the users’ settings, the best model sets are then selected and returned. The implementation allows the model set retrieval process to be largely customized to the users’ liking. Not only is it possible to determine the prediction horizons or the window sizes of the demanded models, but the user is also able to select a preferred top-k algorithm and a performance metric that is used to select the best models and model sets. This work has implemented one naïve top-k algorithm along with the two more sophisticated algorithms FA and TA that make the selection of the best objects more efficient. The user is able to balance performance and resource awareness to their will, by assigning different values to the two implemented weights \texttt{perfWeight} and \texttt{AMSWeight}. By determining an aggregation function for the QSLs inside a model set, the user is left with additional options regarding the assessment of resource awareness inside a model set.


Without having established a theoretical foundation, this implementation however would have been not possible. Researching different ideas on how to represent concepts in a correct but also easily understandable way probably took up the most amount of time and effort spent on this work. This project explored several proposals on how to measure and assess resource awareness, discussing their advantages and disadvantages and explaining why certain alternatives were chosen. By introducing the number of features a model uses as a form of intra-model resource awareness, the degree of efficiently utilizing limited resources can be assessed before forming any model sets. The distinction between intra-model and inter-model resource awareness helps the model set creation process to be more precise and less redundant. In addition, the user can make more accurate observations about the resource utilization of a model set, by not only looking at the number of shared characteristics like window size or features within a set but also taking the number of used features into account.

Similarly, the concept of the implemented prediction horizon turned out to be a crucial part of the whole project. While in earlier stages of the implementation, both the training pipeline as well as the model set retrieval system were working as intended, the true predictive values of the models were questionable. Models that were only predicting the parking occupancy value for the current time slot, would be of little use for actual parking prediction. In addition, the implemented 24 hour-shift would only produce predictions for the next day, which turned out to be rather inaccurate as well as unpractical for actual usage. The idea of introducing a prediction horizon to the system because of these circumstances makes predicting parking availability much more accurate and realistic. The created models are now able to predict parking availability in practically useful time intervals without having to compromise for prediction performance. Ultimately, these introduced concepts lead to a more sophisticated system that is focused on producing valuable results for real-life applications.

The developed system mainly acts as a form of multi-object optimization by weighting the performance of models or model sets against their resource performance. This aligns also with the main aim of this work: To balance performance against resource awareness in order to optimize the model set selection process. However, the implemented system can also be used as a constrained optimization problem by utilizing the setting \texttt{combineSameFeatures} in the API calls \cite{feurer2019}. Instead of using multiple metrics (i.e. performance and resource awareness) for the model set selection, the constraint of only using model sets that have the same feature set could be set by activating \texttt{combineSameFeatures} and setting both weights to 1, indicating a 100\% weight on the performance metric. Thereafter, the system would look for the best-performing model sets, under the condition of only considering model sets that share the same features. This shows the flexibility of the implemented system and how requests can be highly customized to the user's demands.

While the presented modules - both training pipeline and model set retrieval system - were developed with a specific use case in mind - mainly the PAP use case - the conclusions derived from this study can and should be applied to other contexts in the field of machine learning model selection. Hence, the provided findings can also be applied to the CAR use case to train models, create model sets, and eventually select the best model sets using a top-k algorithm.\\