\chapter{Implementation}\label{chap:implementation}

The following chapter will give an overview of how both the latest version of the training pipeline as well as the model set retrieval system was implemented, how the user can customize the retrieval process by changing certain settings, and what challenges were faced during the implementation process.



\section{Model Database}

As mentioned in the previous chapter, there were many fundamental things to improve upon the already existing training-pipeline, when the development phase for this project was first started. The first step was to revise the model attributes by critically thinking about what information a machine learning model in the database should contain. As a result, the following attributes were added to the models in the database:

\begin{itemize}
	\item \texttt{model\char`_id}: Simple serial integer value that serves as primary key and makes it possible to refer to every created model in a precise way.

	\item \texttt{developer}: Attribute that makes it apparent, who trained and stored a specific model.
\item \texttt{created\char`_time}: Timestamp of when the model was successfully stored in the model database. 
\item \texttt{model\char`_size\char`_in\char`_bytes}: Integer value of the memory size of that specific model in bytes. This value is calculated by returning the length of a byte array that is filled using a ByteArrayOutputStream on the content of the classifier.
\item \texttt{training\char`_weeks}: Shows how many weeks’ worth of preprocessed training data was used to train a model. In the previous version of the training pipeline, one had to specify the amount of data entries (and therefore rows of raw parking data) that should be used for training. With the rework, the user now specifies the training data size in weeks instead. It is important to note, that for a fixed trainingWeeks value, a small window size value leads to more data entries used, than a big window size.
\item \texttt{window\char`_stride}: For the sake of completeness the window stride was added as a model attribute and depicts the time difference between the start points of two consecutive data entries used to train a model. In the present use case, the window stride is always the same as the window size.
\item \texttt{start\char`_of\char`_training\char`_data}: Depicts the month and year of the first entry of training data that was used for a model.
\item \texttt{prediction\char`_horizon}: The newly implemented prediction horizon of a model in minutes. Further information about this metric will follow in the next chapters.
\end{itemize}

In addition, the following attributes are reworked versions of previously implemented attributes. They were changed, either to be more comprehensible and uniform with the rest of the projects’ terminology or to refactor the actual values to be better tailored to the implementations’ functionalities.

\begin{itemize}

\item \texttt{model\char`_name}: Instead of choosing an arbitrary name for a model, the model\char`_name attribute was reworked to automatically give a model a standardized name based on its specifications, which makes it easy to extract important information without having to look up each metric separately. The syntax for the naming convention is as follows: \\\texttt{Classifier-Features-WindowSize-TrainingDataSize-ParkingLotID- \\PredictionHorizon-potentialSpecialProperty}. The features are separated by commas and are indexed as seen in \autoref{indexfeatures}.


% Indexing of features
\begin{table}[h]
\centering
    \begin{tabular}{ l | c}
        \toprule
\textbf{Features} &
\textbf{Index}      
 \\\midrule

temperature & 0 \\\hline
humidity & 1 \\\hline
weekday & 2  \\\hline
month & 3 \\\hline
year & 4 \\\hline
time\char`_slot & 5\\\hline
previous\char`_occupancy & 6 \\
        \bottomrule
    \end{tabular}
\caption{Indexes of the features used for training} \label{indexfeatures}
\end{table}


\item \texttt{training\char`_data\char`_size}: Previously called \texttt{table\char`_length}, this attribute states the total number of rows of preprocessed training data that were used for the training of this model. The value is calculated using the following formula: $training\char`_weeks \cdot 7 \cdot \frac{1440}{window\char`_size}$.
\item  \texttt{features}: Previously called \texttt{attributes}. Provides information about the features that were used to train this model. Instead of showing “all”, if every possible features was used in training, in the overworked version now every single feature is listed for better readability. 
\item  \texttt{classifiers}:  The same improvements that were underdone for \texttt{features} were applied to the classifiers attribute for better readability. 
\item  \texttt{accuracy}, \texttt{mae}, \texttt{mse}, \texttt{rmse}: A models performance metrics. In the initial version of the training pipeline, these metrics were calculated and put into one single column, making it unwieldy to process further in the top-k algorithms (As an example, in previous versions of the top-k retrieval system, the accuracy values had to be extracted using a \texttt{re.search} function on the string that contained all the metrics, and then converted to a float value). By making the metrics atomic, they can be accessed more directly without wasting computational resources. Another advantage is the better readability for the user.
\item \texttt{space\char`_ids}: Previously called \texttt{slotids}.
Indicates which parking spaces were considered for training. In the executed pipeline-runs, every parking space was selected.
\item  \texttt{model\char`_content}: Byte array of the model. In the original training pipeline the content of the model was split into four different attributes, one per classifier. As there is only one classifier used per model, this was changed to always display the content in this attribute.
\end{itemize}

The following attributes were mostly adopted directly from the previous implementation, but will be introduced anyway to give context about their meaning:

\begin{itemize}
	\item \texttt{parking\char`_id}: The ID of the parking lot the model is trained on. In the current state, the parking lots 38 and 634 are available.
\item \texttt{window\char`_size}: The window size of the data entries used for training. This value describes how many minutes of parking activity should be summed up into one row of training data in the preprocessing phase. The models that were trained using a prediction horizon have a window size of 1 or 5.  
\item \texttt{trainingdataproportion}: Shows the proportion of the training data in relation to the test data. A value of 0.8 for example puts 80\% of the data into training, and 20\% into testing. 
\item \texttt{accuracypercent}: Metric that decides how many percent points the prediction of the model can deviate from the actual target value without being categorized as a wrong prediction. 
\item \texttt{randomforestmaxdepth}: Hyperparameter for Random Forest classifiers. The maximum number of splits each Decision Tree in the Random Forest can make. Assumes a placeholder value of $-1$ if a classifier other than Random Forest is chosen.
\item \texttt{kneighbours}: Hyperparameter for KNN classifiers. The number of neighbors of a data point that should be checked for classification. Assumes a placeholder value of $-1$ if a classifier other than KNN is chosen.
\end{itemize}

The combination of these attributes made it straightforward to both filter for models when searching for something specific in the database, as well as process relevant information for the retrieval system.




\section{Training Pipeline}

With the structure of the model database explained more insight into the training pipeline can now be given. The functionality of the original model trainer was to create single models by specifying their properties in a \texttt{config.properties} file. After a new \texttt{Settings} object has been created that reads the \texttt{config.properties} file through an \texttt{InputStream}, a connection to the model database was established. Subsequently, the raw parking data was queried from the database and then preprocessed according to the specified properties. The preprocessed data was then used to build, test, and finally store the model in the model database. The process was then ended. It was apparent that simply implementing a repeating sequence of this process was not sufficient for a working training pipeline. Especially the recurring opening of new connections to the database was an obvious thing to be optimized. By restructuring the way the function \texttt{createDBConnection} is called, not only the problem of only establishing five connections at a time could be bypassed, but more prominently vast time savings could be accomplished. 

The general functionality of the training pipeline is based on several nested for-loops, each referring to one metric that is to be changed. Therefore, one for-loop is for example changing a predefined variable that stores the window size that should be iterated over. In order to make an insight into, as well as possible changes to the values in an uncomplicated manner, most of them have been stored in \texttt{HashMaps}. A \texttt{HashMap} may store additional information as well, and so the \texttt{windowSizeMap} also contains the values of \texttt{trainingWeeks} that are attached to the window sizes. In order to provide the model trainer with those changing properties of each new model that was to be built, a new function \texttt{changeValues} was implemented. Using a \texttt{FileOutputStream}, the values of the iteration are filled into the properties file, which was renamed to \texttt{training.properties}. During several test runs of the model trainer, however, it was noticeable that the changes to the properties file only came into force one iteration after the actual change. The reason for this was that changes in the \texttt{FileOutputStream} were only reloaded after each iteration. The \texttt{Settings} class therefore had to be modified in a way, so that it takes the already changed \texttt{Properties} object as an input parameter. The afterward created \texttt{ModelTrainer} object then directly refers to the changed values of the current iteration. 

A first longer test run led to the fixing of some minor problems, one being the handling of double-training the same models: The original model trainer was built in a way that encoded the features a model should be trained on by assigning each feature to a number. Deselecting all features, let to the training of all of them. Although, by selecting every number, the same result was achieved. Such unexpected behavior was adjusted by modifying the for-loops. More importantly, however, another inefficiency was addressed in this development stage. As the models mainly used the same subset of training and testing data, the \texttt{preprocessing} function was operating on the same data each time a new model was created. It was then decided to outsource the preprocessing phase into its own class \texttt{Preprocessor} and define a new database table that would store the preprocessed data for it to then be accessed directly by the model trainer as needed. Essentially, a second pipeline was therefore created that would take care of preprocessing data in reference to certain properties that have been set beforehand. The general procedure in this new preprocessing pipeline is very similar to the way the training pipeline works: The user uses the \texttt{HashMaps} to set up the parking lots and the window sizes that should be preprocessed on. A \texttt{changeValues} function that is similar to the one used in the \texttt{ModelTrainer}  then changes the values in a \texttt{preprocess.properties} file via an \texttt{FileOutputStream}. After that, the raw parking data is queried from the database and is then preprocessed by segmenting it into windows of the previously set size. The window size is the same as the window stride, leading to sliding, non-overlapping windows \cite{dehghani2019}. The preprocessed data containing parking, time, and weather information is then put into SQL statements which are finally executed every 1000 rows. By using a batch size of 1000, long waiting times and potential errors by creating very large statements can be avoided. In order to correctly select the required type of preprocessed data later in the training pipeline, based on the \texttt{pID} and window size of the model that is to be built, it was decided to include this meta-information in the table containing the preprocessed data as well. This was first done through an attribute called \texttt{context} that contained both \texttt{pID} and the window size but was later split up into atomic attributes to be in line with the norm of designing database management systems and to avoid inconstancies \cite{silberschatz2010}.

The attributes of the preprocessed data mostly represent the features the models can be trained on: As mentioned before this includes parking information (\texttt{previous\char`_occupancy}, \texttt{occupancy}), weather information (\texttt{temperature}, \texttt{humidity}), and time information (\texttt{weekday}, \texttt{month}, \texttt{year}, \texttt{time\char`_slot}). \texttt{Time\char`_slot} has been called \texttt{timeHorizon} in the previous version of the model trainer and was therefore renamed in order to avoid misunderstandings. It describes the number of the “slot”, a data entry refers to, by dividing up the 24 hours of a day into time frames of the same size as the windows used for preprocessing. For example, the time slot 2 for a window size of 5 would be the time window from 00:10 AM until 00:15 AM (as the encoding of the first time slot in a day starts with 0). Two other attributes were also added: \texttt{period\char`_start\char`_time}, which is a timestamp of the period it is referring to, and \texttt{shift24h}, which will be discussed in the next subchapter.  

After a full run of the preprocessing pipeline, all available raw parking data is available and ready to be used for model training and testing for the desired window sizes, even if not every data point is actually used for model building (e.g. because of a small \texttt{training\char`_weeks} value). Besides the vast time and computational resource savings that are accomplished this way, the preprocessing pipeline also has another important advantage: With it, it is possible to select and pinpoint the exact number of rows of the preprocessed data and therefore the exact time span of the training data used, by utilizing \texttt{training\char`_weeks}. Without the \texttt{Preprocessor} class, the only option to specify the amount of training data used was through a metric called \texttt{tableLength}, which described the number of rows of raw parking data to be used for preprocessing. Because a varying \texttt{pID} and window sizes lead to a different number of preprocessed data rows, however, this way of selecting the size of the training and test data is more imprecise. Having already established preprocessed data ready, makes it possible to know exactly how many data points were used in the training or testing of a certain model. Another addition that was implemented into the training pipeline was the use of feature scaling, mainly normalization and standardization. This was done using \texttt{Filters} of the Weka library. A normalize or standardize filter could simply be applied to the training- and test-dataset in order to scale the values in a preferred way. The normalization filter rescales all features to values between $0$ and $1$ while standardizing the data sets the mean value to 0 and the standard deviation to $1$. The results of using feature scaling in the training process will be discussed in \autoref{chap:evaluation}.



\section{24 Hour Shift}

A central part of the implementation phase that led to numerous challenges and reconstructions was the so-called 24 hour-shift. It originated from the idea, to train models that would predict the parking occupancy for the same time slot but 24 hours in the future. Up until that point, models could only really “predict” the occupancy value for the time slot the model was run in. Let’s take a model \texttt{dt-1,5,6-60-4-38} as an example. As described earlier, one can derive the most important information about the models’ metrics from its name. In this example, the model works on a Decision Tree classifier, it was trained on the features \texttt{humidity}, \texttt{time\char`_slot} and \texttt{previous\char`_occupancy} using a window size of 60 minutes and four weeks’ worth of training data. Furthermore, it predicts values for parking lot 38. Using the window size and the \texttt{training\char`_weeks} value, it is evident that $(24 \cdot 7 \cdot 4 =) 672$ 
instances were used for training. One of those instances may look like this:

% Exemplary Instance Table
\begin{table}[h]
\centering
    \begin{tabular}{  l  l  l  l }
        \toprule
\textbf{humidity}      
& \textbf{time\char`_slot}   
& \textbf{previous\char`_occupancy}
& \textbf{occupancy} \\\midrule

71 & 14 & 63.7 & 59.3\\
        \bottomrule
    \end{tabular}
\caption{Exemplary instance for model \texttt{dt-1,5,6-60-4-38}} \label{fig:instance}
\end{table}


The first three columns are the features that are used to train the model, while \texttt{occupancy} is the target variable. \texttt{Previous\char`_occupancy} points to the occupancy value of the previous window (so in this case, time slot 13), while all other features display values of the current window. If this exemplary model was put into production it would therefore take the current humidity (for example by fetching live data from a nearby weather station), the current time slot (simply by knowing the current time), and the occupancy value from the previous time slot (by looking it up in a database for example), and then predict the occupancy value for the current time slot using those three features. For simplicity, let’s assume that the current values correspond to the values in the instance shown in \autoref{fig:instance}. Being in time slot 14 (with a window size of 60, at the time of prediction it would therefore be between 14:00 and 15:00), the parking lot would be occupied for 59.3 \% in this hour on average. The model now predicts how occupied the parking lot is on average in the current time slot. The benefit of this prediction highly depends on the exact point in time when the prediction is being done: If the model predicts the occupancy value for time slot 14 at the beginning of said time slot (e.g. at 14:03) there certainly might be value to the prediction, as the majority of the current time slot has still not happened. However, if the prediction is done near the end of the current time slot (e.g. at 14:58) the informative value might be completely negligible: The prediction corresponds mainly to the past 58 minutes, and while it still might represent the actual current occupancy to some extent, can’t really be considered a prediction in the proper sense. 

During the implementation phase, it became apparent that this behavior was not aligned with the desired use case of the PAP system. While the models do predict in the sense of machine learning, the prediction cannot really be considered as such, as the use case requires something else: The predicted parking lot occupancy in a specific amount of time. It was then decided to implement the previously addressed 24 hour-shift. For this, a special type of preprocessed data first had to be stored using the preprocessing pipeline. Instead of using the time information of the current time slot as features, the time features of the previous day, but the same time slot (therefore having a time difference of 24 hours) would be looked up and put into the corresponding column of the instance. For this, the previously introduced attribute \texttt{period\char`_start\char`_time} was used: A new timestamp was created using the \texttt{minusHours(24)} function from the \texttt{Java.time} library. Subsequently, the \texttt{weekDay}, \texttt{month} and \texttt{year} features were adapted to this new timestamp. Note, that the \texttt{time\char`_slot} feature was not manipulated, as a shift of exactly 24 hours means the time slot stays the same within a day. Further, the \texttt{previous\char`_occupancy} value was set to -1. The reason for this was, that speaking from a practical view, using \texttt{previous\char`_occupancy} as a feature in production wouldn’t be of much sense when using the 24 hour-shift: There was no way of knowing for certain, how high the occupancy was in the time slot before the time-to-be-predicted. It was therefore decided that \texttt{previous\char`_occupancy} would be unavailable for all models trained on the 24 hour-shift. With the time information features pointing to a timestamp 24 hours before, the occupancy value staying the same and the previous occupancy value essentially being set to null, the weather information features were not altered. This also had a practical reason: It was reasoned that when using the 24 hour-shift models in production, the weather information of the point in time to be predicted could be gathered relatively easily by using a weather forecast. The limitations of this approach and other possibilities for handling the weather data will be discussed in \autoref{chap:conclusion}. Models that were trained using the 24 hour-shift, are marked with a \texttt{-24h} suffix in their \texttt{model\char`_name} attribute. In concision, by implementing the 24 hour-shift, it was possible to have a model predict the occupancy value of a point in time in the future, rather than predicting only the current time slot.
 
 Another addition to the training pipeline was the option to change hyperparameters for certain classifier algorithms. The goal was to both change the number of neighbors for the KNN algorithm, as well as the maximum depth of the Random Forest algorithm to introduce more diversity into the model database and identify potential changes in performance. While these hyperparameters could be changed manually in the original version of the \texttt{ModelTrainer}, an automatic altering of the values was introduced by implementing the function \texttt{changeHyperparameters} that works very similar to the previously introduced function \texttt{changeValues}, and is called by using another for-loop in the main method, that is skipped if a classifier that doesn’t have any changeable hyperparameters (Decision Tree and Linear Regression) is being built. For storing the values of \texttt{kNeighbours} and \texttt{randomForestMaxDepth} new \texttt{HashMaps} were defined, just like for the other values that are being changed in the training pipeline. 
 
 
 
 \section{Running the Pipeline}
 
 After all these changes were applied to the training pipeline, the first proper run of the training pipeline was carried out. Here, the aim was to create as many diverse models as possible, using the previously introduced attributes such as the window size or the training data size as settings to adjust. The created models can be accessed in the table \texttt{niklas\char`_trained\char`_models} which is located in the universities \texttt{postgreSQL} database. The following attributes have been changed systematically so one model with each possible combination was trained. Each metric’s name is followed by the values that each attribute was given in the training run:
 
 \begin{itemize}
 	\item \texttt{parkingLot}: 38 or 634.
\item \texttt{windowSize}: 10, 30 or 60 minutes.
\item \texttt{trainingWeeks}: (training data size in weeks worth): 1 or 4 weeks. 
\item \texttt{classifier}: Decision Tree, Random Forest, Linear Regression, KNN.
\item \texttt{features}: \texttt{temp}, \texttt{humidity}, \texttt{weekday}, \texttt{month}, \texttt{year}, \texttt{previous\char`_occupancy}.
 \end{itemize}
 
 The feature \texttt{timeslot} was included in every built model. In addition, special models containing the following metrics were trained:
 
 \begin{itemize}
 	\item \textbf{24 hour-shift}: Only for window size of 60 minutes; feature \texttt{previous\char`_occupancy} was always deselected in the case of a shift. 
\item \textbf{Feature Scaling (Normalization)}: Only for \texttt{windowSize} of 60 and \texttt{training\char`_weeks} of 4.
\item \textbf{Hyperparemeters}: For Random Forest and KNN classifiers, changes in hyperparameters were considered. Each Random Forest model exists in variants of \texttt{maxDepth} = 1, 5, and 20. Each KNN model exists in variants of \texttt{kNeighbors} = 3, 19 and 33.
 \end{itemize}
 
 While changing the following metrics is possible using the current version of the training pipeline, they were not altered in the training run. The metrics’ name is followed by the value that has been chosen for this run:
 
 \begin{itemize}
 	\item \texttt{trainTestStrategy}: Test after Train (using the first data points as training, the remaining for testing).
\item \texttt{trainingDataProportion} (proportion of training data in relation to test data): 0,8.
\item \texttt{accuracyPercent} (tolerance in percent for accuracy calculation): 1.
\item \texttt{slotsIDs}: All parking slots were considered for training.
 \end{itemize}
 
 Using these combinations, a total of 4603 models have been trained, tested, and stored in about 8,5 hours. Models using the Random Forest algorithm used up the most time during building, with an estimated average of about 30 seconds per model. Observations regarding the performance of the generated models will follow in \autoref{chap:evaluation}.
 
 
 
 \section{Prediction Horizon}

After the first proper model training pipeline has been successfully run, discussions about the true meaning of the metric \texttt{windowSize} arose. In various discussions and debates, it was thought about how to accomplish a proper parking lot occupancy prediction, without having to use the procedure for the 24 hour-shift introduced before. Further discourse showed that the original idea behind \texttt{windowSize}, was not only to segment the raw parking data into windows of this size, but also use it as a way of predicting future occupancy values. Furthermore, discussions lead to the realization that higher window sizes like 30 or 60 minutes might not lead to accurate results for predictions that lie in the future for only a short time, like 10 minutes. It was decided that the time difference between the point of prediction and the time-to-be-predicted should be called prediction horizon. Subsequently, the following conclusions were made: 

\begin{enumerate}
	\item Training models using a smaller window size (like 1 or 5 minutes) will lead to more fine-grained preprocessed data that might produce better results.
\item Using these models trained on small window sizes without any form of future prediction aspect will have high accuracy but very low informative value. (E.g. “predicting” the average occupancy for this very minute is not of value for the user: Current occupancy could easily be viewed through live data from sensors etc.)
\item Combining the small window sizes with some sort of shift might produce accurate and meaningful predictions. However, if the procedure for the existing 24 hour-shift is to be reused, many preprocess-runs must be made for each combination of window size (e.g. 1 and 5 minutes) and time horizon (e.g. 10, 30, and 60 minutes). This might make future training unflexible, as data first has to be preprocessed for every different prediction horizon and window size.
\end{enumerate}

Following these conclusions, it was decided that by utilizing preprocessed data segmented on small window sizes and applying a prediction shift after the preprocessing, an agreement between accuracy, meaningful results, and flexibility could be met. Therefore, a new variable called \texttt{predictionHorizon} was introduced to the implementation. Therefore, the time difference between the time of prediction and the time of \texttt{occupancy} to be predicted could be represented much clearer and more flexibly. The values that \texttt{predictionHorizon} is to adopt, are stored in \texttt{HashMaps}, just like for the other variables. As the preprocessing is not done specifically for every different prediction horizon beforehand, more data than the actual specified data size for training and testing - as set in \texttt{training\char`data\char`_size} - has to be gathered from the database. In total, the number of data points required for training and testing equates $\texttt{training\char`_data\char`_size} + \texttt{predictionHorizon}$.


The reason for this is the way the data instances for training and testing are created: While most values stay the same (i.e. weather and time information), the \texttt{previousOccupancy} is set to the \texttt{occupancy} value. Then, the \texttt{occupancy} value is set to the \texttt{occupancy} value from the next data instance. The handling of the training and testing data in this updated version is therefore quite different from the data handling in the 24 hour-shift. Rather than discarding the \texttt{previousOccupancy} feature, it is now used to correspond to the \texttt{occupancy} value of the current time slot. The target variable now is set to the occupancy value in \texttt{predictionHorizon} minutes. This value is fetched by looking at the index of the current row of the training/test data and adding the \texttt{predictionHorizon} value divided by the window size value by it. For a better understanding, the actual implementation for handling the prediction horizon in code is shown in \autoref{codepredhor}.


\begin{figure}
\begin{lstlisting}
// Instance handling for prediction horizon
// Prev. Occ set to current Occ; Occ set to the value in predHor.-Minutes 
// Other values stay the same
for (int i = 0; i < trainingDataSize; i++) {
	res.row(i).setDouble("previousOccupancy", res.row(i).getDouble("occupancy"));
   res.row(i).setDouble("occupancy", res.row(i+(settings.predictionHorizon/settings.windowSize))
            .getDouble("occupancy"));
}
\end{lstlisting}
\caption{Modification of training data regarding the prediction horizon}
\label{codepredhor}
\end{figure}


Utilizing this newly created way of referencing the time that is to be predicted, a second large run of the training pipeline has been executed. The changing metrics in this run stayed the same as for the first run with the following exceptions: Following conclusion 1 from above, the models of this run were trained only using window sizes of 1 or 5 minutes. Additionally, all models were trained with the newly introduced variable \texttt{predictionHorizon} being either 10, 30 or 60 minutes long. A short summary regarding the performance of the models created in the second run of the training-pipeline will also follow in \autoref{chap:evaluation}.



\section{Retrieval System}
 
The general aim of implementing the model set retrieval system was to provide the user with a highly customizable option to retrieve models and model sets that are tailored to their needs and the specific use case, whilst relying on different algorithms that have been well established in prior scientific work. This subchapter aims to give an overview on the functionalities of the retrieval system as well as the development process.

After setting up the development environment, the first functions that were implemented were simple READ- and CREATE endpoints called \texttt{select\char`_direct} and \texttt{select} that were introduced before. As a next step, the actual retrieval system was to be implemented. To create a basic working function, it was decided to first focus on retrieving only single models instead of model sets, as to that point of time in the development phase there has not been made a choice on how to approach model set creation. This first created retrieval function is called \texttt{topk} and starts by reading and storing all the user’s inputs. Subsequently, the environment variable containing the URL and login information for the \texttt{postgreSQL} database is loaded and a corresponding connection to the model database is being established. Then, a SQL statement containing information about the relevant machine learning models is prepared and executed. Once the models are obtained from the model database and stored in the \texttt{Table} datatype, each model is prepared so both the performance as well as the resource awareness metric are easily accessible, storing the corresponding values in columns called \texttt{1} and \texttt{2}. These generic names were chosen on purpose in order to make the processing of other data easier without having to change anything in the code itself. The models are then further curated by normalizing the metrics, creating tables each containing only one metric, sorting them, and storing these tables in a \texttt{dictionary} datatype. At this point, the lists of models are ready to be further processed by the top-k algorithms. Therefore, in the next step, the algorithm that was selected by the user is run, producing a list of the top-k models, that are then converted into JSON together with relevant meta information.

Before going into detail on how the functions for model set retrieval were implemented, \autoref{settings} will give an overview of what settings the user can make to customize their retrieval request. The description of the settings is followed by the possible values the settings can take on in the current implementation. 


%  Retrieval System settings table
\begin{table}[h]
\centering
    \begin{tabular}{  l  p{7cm}  p{3cm} }
        \toprule
\textbf{Setting}      
& \textbf{Description}   
& \textbf{Values} \\\midrule

pID & The parking lot ID that the model set should predict for & \texttt{38} or \texttt{634} \\\hline
windowSize & The window size option(s) of the models inside the model set & \texttt{1}, \texttt{5} or both \\\hline
perfMetric & The performance metric to be used to calculate the performance score & \texttt{acc}, \texttt{mae}, \texttt{mse}, \texttt{rmse} \\\hline
k1 & Number of models to consider for model set creation per prediction horizon. E.g., if k1 = 3, for every prediction horizon the top 3 models will be chosen to then further create all possible combinations of model sets. When having two prediction horizons, this means $3 \cdot 3 = 9$ different model sets are created & \texttt{1} to \texttt{n}, where \texttt{n} is the maximum number of models found in the model database \\\hline
k2 & Number of model sets to be returned to the user. Out of all created model sets in the first top-k round, the top $k2$ are then selected. Setting k2 to \texttt{max} will automatically return all created model sets. E.g., when having three different prediction horizons, \texttt{max} will return $3 \cdot 3 \cdot  3 = 27$ different model sets & \texttt{max} or \texttt{1} to \texttt{n}, where \texttt{n} is the maximum number of model sets that can be created using k1 models \\\hline
predHor & The different prediction horizons to be considered in the model sets & \texttt{10}, \texttt{30}, \texttt{60} or a combination of those \\\hline
perfWeight & The weight of the Performance Score in relation to the Resource Awareness Score. E.g. a value of 0.8 will compute the Model Score using 80\% of the Performance Score and 20\% of the Resource Awareness Score & Value between \texttt{0} and \texttt{1} \\\hline
AMSWeight & The weight of the Aggregated Model Score in relation to the QSL Score. E.g. a value of 0.8 will compute the Model Set Score using 80\% of the Aggregated Model Score and 20\% of the QSL Score & Value between \texttt{0} and \texttt{1} \\\hline
algorithm & The top-k algorithm to assess the top $k$ items for both rounds & \texttt{naive}, \texttt{fagin} or \texttt{threshold} \\\hline
combineSameFeatures & Option that makes it possible to only create model sets where all models use the same features & \texttt{true} or \texttt{false} \\\hline
calculateQSL & Aggregation function for determining the overall QSL of a model set & \texttt{min}, \texttt{max} or \texttt{avg} \\

        \bottomrule
    \end{tabular}
\caption{Settings in the Retrieval System} \label{tab:settings}
\end{table}


The implementation for the model set retrieval is largely similar to the \texttt{topk} function up to the point of starting the top-k algorithms. As model set retrieval requires multiple different prediction horizons inside a single request, the selected algorithm has to be run separately for each prediction horizon. For instance, FA chooses the best models with a prediction horizon of 10, 30, and 60 minutes one by one and puts each list into another list named \texttt{result}. To follow the idea of creating actual sets of models out of those selected single models, the helper function \texttt{create\char`_combinations} is called. This function then generates all possible combinations of models, using one model per prediction horizon. If \texttt{combineSameFeatures} is set to true in the API call, \texttt{create\char`_combinations} only creates sets of models that use the same features. At the time of implementation, this was the only way to ensure some sort of inter-model resource awareness, which was later complimented by the QSLs. The implications of the \texttt{combineSameFeatures} setting in regard to the QSLs will be discussed in \autoref{chap:conclusion}.

After all possible model combinations have been created, the sets are then further prepared for the second algorithm call. For that, a new \texttt{dictionary} containing both the model sets themselves as well as meta-information like identifications and scores is created. The relevant scores in this step are the Aggregated Model Score and the QSL Score which are ascertained or calculated here. The determination of the QSLs follows the user's settings for the QSL aggregation rule. After the relevant scores are established, the model sets are again split into two lists, one for each scoring metric. The labels for the two lists are \texttt{1} and \texttt{2} again to make the processing in the top-k algorithms clearer, however, the underlying metrics are now the Aggregated Model Score and the QSL Score. After each list has been sorted and the specified top-k algorithm is run again, the result is converted into a JSON readable format and finally returned to the user.

To run the top-k retrieval system, the user must first start the Flask app whilst being connected to the university's network (directly or by VPN). To do that, the user has to change into the directory \texttt{topkretrieval}. Then, the virtual environment for the implemented system must be entered by executing \texttt{source .venv/bin/activate}. Afterwards, the app can be started by running \texttt{flask run --port 8000}, or any other arbitrary port. The retrieval system can then be accessed by using an API client like Insomnia and reaching the endpoints stated above. For instance, to reach the endpoint for model set retrieval, a POST statement would have to be sent to \texttt{http://127.0.0.1:8000/api/topk/modelsets} appending the necessary JSON data explained in \autoref{tab:settings}.

In summary, both the preprocessing- and training-pipeline as well as the top-k retrieval system have been designed and implemented in a way that makes it possible for future users to simply preprocess, train, and retrieve models and model sets without having to do any major changes to the code. The implemented modules are ready to use once the desired settings and metrics are put in the pipelines corresponding \texttt{HashMap}s or the CRUD statement of the API endpoint one is calling.




