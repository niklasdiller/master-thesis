\chapter{Related Work} \label{chap:relatedwork}

This chapter aims to give an overview of the theoretical background of the topics and concepts that will be examined and discussed in this work. Before going deeper into the topic of the model selection process and how to optimize it, certain keywords need to be specified in order to have a common understanding about reoccurring terms. For this, the definitions of Zhi-Hua Zhou are used \cite{zhou2012}. The process of applying a learning algorithm to data can therefore be considered as training. By doing that, a model or a so-called learner is generated, that is then used to predict the outcome of a specific problem. A distinction can be made between supervised and unsupervised learning. The former uses predefined labels or values as possible outcomes and is used in the use cases of both parking availability prediction as well as cattle activity recognition. In this case, the model can also be called predictor. As the labels in the cattle activity recognition use case are of categorical type, another name that can be used for the model is classifier. Unsupervised learning on the other hand is working without the use of any fixed labels or categories. Its aim is to uncover specific traits and structure in the data.

Model selection itself now describes the process of selecting the best learning algorithm – for example Linear Regression or Decision Tree – and tuning its corresponding parameters like Data Preprocessing or maximum depth \cite{zhou2012}. To improve both the performance and the usability of the model management framework, this thesis will focus on the use of ensemble methods. This chapter will give an introduction to several theoretical concepts and paradigms. First, ensembles in machine learning will be introduced. Next, this chapter will give an overview of the use cases that this work refers to. Subsequently, both resource awareness as well as performance will be presented.


\section{Ensembles}


Given that the output of a learning algorithm that works on a certain training set is considered a classifier, an ensemble can generally be defined as a set of classifiers. Each individual output of these classifiers is then combined in order to achieve one decision that predicts the problem in a better way, than the single classifiers could have done on their own and in an isolated way \cite{dietterich2000}. Ensembles can generally be divided into homogenous and heterogenous ones. While the former consist of models that have been trained on the same kind of machine learning algorithm, the latter is constructed using different kind of algorithms in the ensemble, for example both decision trees and SVMs \cite{zhou2012}. Three main reasons can be identified on why ensembles are often working better in producing high quality predictions in comparison to individual classifiers \cite{dietterich2000}: First off, an ensemble gives a statistical advantage in reaching the true hypothesis of the observed problem. By averaging the classifiers in use, the distance from the true hypothesis to the hypothesis used by the model can be reduced. Secondly, computing the unknown true hypothesis is often difficult. That is why using distinct approaches coming from different classifiers could help to come closer to the true function. Lastly, representation plays a role, as it is often not able to represent the true hypothesis by using the available hypothesis space. By combining different classifiers, this space of representable functions can be widened. Rokach identifies four building blocks that make up an ensemble \cite{rokach2010}: A training set that the model will learn on, a base inducer that forms a classifier by obtaining the training set, a diversity generator that provides the required variety in the classifiers and lastly a combiner that merges the classifications of the models into one single prediction. In this work, the 

Following, a more detailed look into individual ensemble methods will be made. By doing that, the ensemble methods will be divided into dependent and independent methods.  

\section{Dependent and Independent Methods}

% TODO Add remaining related work text -> Noch einige rote Stellen in Word.